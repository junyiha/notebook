---
category: LLM
date: 2025-04-28 09:00:00 +0800
layout: post
title: ollama 入门
tag: LLM
---
# 简介

+ ollama工具学习

<!--more-->

# 基本认知

+ ollama，是一个开源的LLM(大型语言模型)服务工具，用于简化在本地运行大语言模型，降低使用大语言模型的门槛，使得大模型的开发者，研究人员和爱好者能够在本地环境快速实验，管理和部署最新大语言模型，包括如DeepSeek，Qwen2，Llama3，Phi3，Gemma2等开源的大型语言模型。支持的大预言模型列表，可通过搜索模型名称查看：https://ollama.com/library

+ 在本地构建的启动流程：
```shell
# 先启动服务器
./ollama serve 

# 在独立的shell中运行一个模型
./ollama run llama3.2
```
+ 使用ollama的方式有
  + 命令行工具
  + REST API
  + ollama-python

+ Ollama api地址为: http://localhost:11434

# ollama 常用的系统环境变量

+ OLLAMA_MODELS：模型文件存放目录，默认目录为当前用户目录
  + Windows 目录：C:\Users%username%.ollama\models，
  + MacOS 目录：~/.ollama/models，
  + Linux 目录：/usr/share/ollama/.ollama/models，
  + 如果是 Windows 系统建议修改（如：D:\OllamaModels），避免 C 盘空间吃紧

+ OLLAMA_HOST：Ollama 服务监听的网络地址，默认为127.0.0.1，如果允许其他电脑访问 Ollama（如：局域网中的其他电脑），建议设置成0.0.0.0，从而允许其他网络访问

+ OLLAMA_PORT：Ollama 服务监听的默认端口，默认为11434，如果端口有冲突，可以修改设置成其他端口（如：8080等）

+ OLLAMA_ORIGINS：HTTP 客户端请求来源，半角逗号分隔列表，若本地使用无严格要求，可以设置成星号，代表不受限制

+ OLLAMA_KEEP_ALIVE：大模型加载到内存中后的存活时间，默认为5m即 5 分钟（如：纯数字如 300 代表 300 秒，0 代表处理请求响应后立即卸载模型，任何负数则表示一直存活）；我们可设置成24h，即模型在内存中保持 24 小时，提高访问速度

+ OLLAMA_NUM_PARALLEL：请求处理并发数量，默认为1，即单并发串行处理请求，可根据实际情况进行调整

+ OLLAMA_MAX_QUEUE：请求队列长度，默认值为512，可以根据情况设置，超过队列长度请求被抛弃

+ OLLAMA_DEBUG：输出 Debug 日志标识，应用研发阶段可以设置成1，即输出详细日志信息，便于排查问题

+ OLLAMA_MAX_LOADED_MODELS：最多同时加载到内存中模型的数量，默认为1，即只能有 1 个模型在内存中

# ModelFile

+ 这是一个类似 DockeFile的哲学设计，通过 ModelFile 可以快速构建一个模型镜像，然后通过 ollama run 命令直接跑起一个模型
```shell
cat Modelfile
# FROM ./vicuna-33b.Q4_0.gguf

# 定制为本地模型
ollama create vicuna:33b -f Modelfile
ollama run vicuna:33b
```

# ollama 导入本地模型

+ ollama 导入模型到本地的三种方式
  + 直接从Ollama远程仓库拉取
  + 通过GGUF模型权重文件导入到本地
  + 通过safetensors模型权限文件导入到本地

+ 引用
  + https://blog.csdn.net/a2875254060/article/details/146008909

# 参考

+ https://ollama.cadn.net.cn/#quickstart
+ https://www.runoob.com/ollama/ollama-tutorial.html
+ https://ollama.readthedocs.io/