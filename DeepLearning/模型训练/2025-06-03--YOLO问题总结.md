---
category: DeepLearning
date: 2025-06-03 09:00:00 +0800
layout: post
title: 图像标注
tag: AI部署
---
## 摘要

+ 图像标注

<!--more-->

## YOLO模型训练后的 best.pt 和 last.pt 区别

+ best.pt 
  + 这个文件保存的是在训练过程中表现最好的模型权重。
  + 通常用于推理和部署阶段，因为它包含了在验证集上表现最好的模型权重，能够获得最佳的性能。

+ last.pt
  + 这个文件保存的是最后一次训练迭代结束后的模型权重。
  + 通常用于继续训练模型，因为它包含了最后一次训练迭代结束时的模型权重，可以继续从上一次训练结束的地方继续训练模型

## YOLO模型训练时，出现 Closing dataloader mosaic

+ close_mosaic:
  + 禁用 mosaic 增强的最后第几个轮次。可以指定一个数值，表示在训练的最后第几个轮次中禁用 mosaic 增强

+ Mosaic数据增强方法
  + 是YOLOv4论文中提出来的，主要思想是将四张图片进行随机裁剪，再拼接到一张图上作为训练数据，这样做的好处是丰富了图片的背景，并且四张图片拼接在一起变相的提高了batch_size，在进行 batch normalization的时候也会计算四张图片，所以对本身batch_size不是很依赖，单块GPU就可以训练YOLOv4

## yolo 早停法怎么实现

在 YOLOv8 中，早停（Early Stopping）是内置机制，只要在训练时指定合适的 `patience`（耐心轮数）参数，就能让训练在验证指标（如 mAP 或 loss）在连续若干个 epoch 内没有提升时提前终止。下面给出具体步骤和示例：

1. **理解 `patience` 参数**

   * YOLOv8 会在每个 epoch 结束后计算一次“fitness”指标（通常是 mAP、precision、recall 等的加权组合），并与历史最优值对比。如果在连续 `patience` 个 epoch 中都没有出现更好的 fitness，就会自动触发早停。
   * 默认情况下，`patience=50`，也就是说如果验证 fitness 在 50 个 epoch 内一直没提高，就结束训练。你可以根据数据集大小、类别数、以及模型收敛速度自行调整此值([docs.ultralytics.com][1], [github.com][2])。

2. **通过命令行实现早停**

   * 如果你直接使用 Ultralytics 提供的 CLI 脚本（通常是 `yolo train …`），只需在命令里加上 `--patience` 参数即可。例如：

     ```bash
     yolo train \
       data=custom_data.yaml \
       model=yolov8s.pt \
       epochs=200 \
       batch=16 \
       patience=10
     ```

     这行命令表示：在最多训练 200 个 epoch，若验证 fitness 在连续 10 个 epoch 里没有提升，就自动停止训练([github.com][2], [blog.csdn.net][3])。

3. **通过 Python API 实现早停**

   * 如果你是用 Python 脚本调用 Ultralytics 的 `YOLO` 类，也同样可以传递 `patience` 参数：

     ```python
     from ultralytics import YOLO

     # 加载模型（可以是 .pt 或 .yaml 格式）
     model = YOLO('yolov8s.pt')

     # 开始训练，指定 patience=10
     results = model.train(
         data='custom_data.yaml',
         epochs=200,
         batch=16,
         patience=10,
         imgsz=640
     )
     ```

     这样配置后，如果在连续 10 个 epoch 里验证集上的 fitness 不再提升，训练会自动结束([blog.csdn.net][3], [docs.ultralytics.com][1])。

4. **如何查看早停触发**

   * 在训练日志中，你会看到类似 “EarlyStopping: no improvement in last 10 epochs, stopping” 的提示。此时模型会保留在“最优 epoch”对应的权重文件（比如 `weights/best.pt`），避免后续过度训练导致过拟合。

5. **调整 `patience` 的建议**

   * 如果数据集较小、noise 较多，可以适当缩小 `patience`（如 5–10），避免模型在噪声上反复“打转”。
   * 如果数据集复杂、或想让模型多找找更优解，也可以适当增大（如 20–50），以免过早停止而错过收敛好时机。

---

**小结**：

* 只要在训练命令或 Python 接口中加上 `patience=<N>`，YOLOv8 就会启用内置的 EarlyStopping 机制；
* 当验证指标在连续 N 个 epoch 中没提升时，训练自动终止，并保留最佳权重。

通过调整 `patience`，你可以很好地在训练速度和防止过拟合之间做平衡。

[1]: https://docs.ultralytics.com/reference/utils/torch_utils/?utm_source=chatgpt.com "Reference for ultralytics/utils/torch_utils.py - Ultralytics YOLO Docs"
[2]: https://github.com/ultralytics/ultralytics/issues/3892?utm_source=chatgpt.com "How you can change patience value when continuous training yolov8 ..."
[3]: https://blog.csdn.net/apple_59275002/article/details/132181112?utm_source=chatgpt.com "UltralyticsYOLOv8训练与验证参数详解-CSDN博客"

## 过拟合和欠拟合 怎么理解

在机器学习和深度学习中，“过拟合”（overfitting）和“欠拟合”（underfitting）是两个经常出现的概念，它们反映了模型在训练数据与未见数据（验证集/测试集）上的表现差异。下面从定义、成因、表现、判断方法以及如何避免/缓解等方面来阐释这两个概念。

---

## 1. 定义

1. **欠拟合（Underfitting）**

   * **直观理解**：模型太“简单”，连训练集本身都学不好，无法捕捉数据的基本规律。
   * **结果表现**：训练误差（training error）和验证误差（validation error）都很高。
   * **本质**：模型的容量（capacity）或表达能力不足，无法拟合数据里的主要特征。

2. **过拟合（Overfitting）**

   * **直观理解**：模型太“复杂”，把训练数据中的噪声或偶然关联也当成了“规律”学进去，因而在训练集上表现极好，但在新数据上泛化能力差。
   * **结果表现**：训练误差很低，但验证误差较高，二者出现很明显的差距。
   * **本质**：模型容量太大或者对训练数据的记忆过度，以至于失去了对未见数据的泛化能力。

---

## 2. 为什么会出现欠拟合或过拟合

### 2.1 欠拟合的原因

1. **模型太简单**

   * 例如：用线性模型（如线性回归）去拟合一个明显非线性的关系，模型本身无法表达目标函数的复杂度。
   * 例：在图像分类任务里，用参数非常少的神经网络（如只有一两层、很小的隐藏单元数）去识别复杂场景，很难提取足够特征。

2. **特征不够/信息不足**

   * 如果输入特征本身不能很好地反映目标类别（比如分类时只用了图像的平均像素强度作为特征），即使模型更复杂也难以学到规律。

3. **训练不充分**

   * 训练轮数（epochs）太少或者优化超参数（如学习率）设置不合理，导致模型还没来得及“收敛”到较优解。此时训练误差没降下来，本质也是欠拟合。

4. **正则化/约束过强**

   * 如果人为在模型里加了很强的正则化（例如 L2 惩罚系数太大、dropout 率过高），就会限制模型权重的更新幅度，导致模型只能简单拟合，无法学到数据的细节。

### 2.2 过拟合的原因

1. **模型太复杂/容量太大**

   * 参数量远大于训练样本量时，模型很容易对训练集“记忆”过度。比如在只有几千张图片时，用上亿参数的深度网络，很容易 memorize 那些样本中的噪声特征。

2. **训练数据噪声或异常样本**

   * 如果训练数据中含有标签噪声（标签标错、误标）以及一些离群点，模型将努力去拟合这些异常，导致学到对新数据无意义的“规律”。

3. **训练轮数/训练时间过长**

   * 当训练轮次（epochs）太多，如果不加以控制（如没有及时使用早停法 Early Stopping），模型会在训练集上把误差压得很低，但此时可能已经开始在噪声点上“过度记忆”。

4. **数据量太少**

   * 样本不足时，模型能用的有效信息较少，一旦模型容量（参数量）很大，就极容易把这有限的数据“啃”得面面俱到，从而丧失泛化能力。

---

## 3. 在训练曲线上如何辨别

通常借助“学习曲线”（Learning Curves）——在训练过程中不断记录训练误差和验证误差随 epoch 的变化，能够比较直观地看出欠拟合/过拟合。

1. **欠拟合情形**

   * 训练误差高且下降缓慢，验证误差几乎和训练误差保持一致，也很高。
   * 曲线示例：

     ```
     训练误差：|\
               | \
               |  \
               |   \
               |    \
               |     \
               ————————> epoch
     验证误差：|\
               | \
               |  \
               |   \
               |    \
               |     \
               —————————> epoch
     ```
   * 训练集和验证集都没学好，说明模型容量或训练强度不够。

2. **过拟合情形**

   * 训练误差迅速下降到很低，而验证误差先下降到某个最低点后又开始回升，两者出现“分叉”。
   * 曲线示例：

     ```
     训练误差：|\__________（很快下降到低值后持平）
               |
               |
               |
               |
               |
               ————————————> epoch
     验证误差：|\_____/‾‾‾‾‾
               |
               |
               |
               |
               |
               ————————————> epoch
     ```
   * 训练集表现优秀，验证集开始出现误差回升，说明模型开始对训练集特征（包括噪声）过度拟合。

---

## 4. 如何改善欠拟合

1. **增加模型的容量**

   * 对于神经网络，增大网络深度或宽度（如更多层数、更多神经元）。
   * 对于树模型（如决策树、随机森林），可以放宽树的深度限制；对线性模型，可以尝试添加多项式特征或使用更复杂的核函数（对于 SVM）。

2. **增加训练轮数/提升优化能力**

   * 增加训练 epoch，让模型有更多机会去拟合数据。
   * 检查并调试超参数：适当提高学习率（如果学习率过小、网络收敛速度慢也会导致欠拟合），或者换用更合适的优化器（如从 SGD 换成 Adam）。

3. **减少正则化强度**

   * 如果使用了 L1/L2 正则化或 dropout，尝试减小正则化系数或降低 dropout 率，以让模型有更大自由度去拟合数据。

4. **改进特征工程**

   * 从输入角度丰富信息：增加更多有用特征、进行特征组合、做多尺度/多角度提取。
   * 对于图像，可以进行更丰富的数据增强，让模型见到更多多样化样本，从而学到更充足的特征。

---

## 5. 如何改善过拟合

1. **增加训练数据量**

   * 这是最有效的手段：更多的数据能让模型学习到更丰富的真实分布，弱化噪声的影响。
   * 如果收集新数据成本高，可以考虑数据增强（Data Augmentation）：对图像做随机裁剪、旋转、色彩抖动、噪声叠加等，让“虚拟数据量”增多。

2. **使用更强的正则化手段**

   * **L1/L2 正则化**：在损失函数中加入权重惩罚，抑制权重过大，从而防止网络过度拟合。
   * **Dropout**：在训练时随机“屏蔽”一部分神经元，迫使网络不要过度依赖某个通道。
   * **Early Stopping（早停法）**：在训练过程中监控验证集误差，当验证误差连续若干 epoch 不再下降时，就提前终止训练。

3. **减少模型复杂度**

   * 选用容量更小的模型：如果之前是 YOLOv8l 或 YOLOv8x，可以尝试降到 YOLOv8s；如果是 ResNet-152，可以考虑切换到 ResNet-50。
   * 对于树模型，可以缩减树深度、控制叶子节点最小样本数、减少基学习器数量（对于集成方法）。

4. **特征选择或降维**

   * 对于高维特征空间，可以用 PCA、LDA 等降维方法去掉冗余维度，减少噪声。
   * 在特征工程中去除与目标相关度低或与其他特征高度共线的特征，也能减少过拟合。

5. **交叉验证（Cross-validation）**

   * 通过 k 折交叉验证，可以更充分地使用数据来评估模型性能，发现是否存在严重的过拟合倾向。

---

## 6. 典型示例与直观类比

1. **拟合一元二次曲线的例子**

   * 假设真实关系是 $y = 3x^2 + \epsilon$，但你只用一阶线性模型 $y = wx + b$ 去拟合，结果即使在训练集上也可能拟合不好，就是欠拟合。
   * 如果你用高次多项式（比如 10 次多项式）去拟合少量带噪声样本，模型曲线会在每个点都“过度弯折”来追踪噪声——训练误差几乎为零，但在新 x 值上预测极差，就是过拟合。

2. **考试答题的类比**

   * **欠拟合**：就像只背了最基础的几页笔记，考试题一多出现一些变形题就不会做。
   * **过拟合**：为了背书，你把所有历年考卷一题都记住了，连错误选项都牢牢记在脑子里。结果一旦考试题型有一点变化，你就不知道如何应用核心思路，反而答不出新题。

---

## 7. 总结

* **欠拟合**：模型“不够强” → 训练误差高、验证误差高 → 解决方案是增加模型容量、增多训练数据/训练时间、减弱正则化、改进特征。
* **过拟合**：模型“太强”且对训练数据记忆过度 → 训练误差低、验证误差高 → 解决方案是增加数据、加正则化、降低模型复杂度、使用早停、做交叉验证。

在实际项目中，往往需要在“欠拟合”和“过拟合”之间做权衡（bias–variance trade-off）。通过不断地观察训练/验证曲线，以及在调参过程中逐步进行对比，才能找到让模型在新数据上表现最优的“黄金中点”。

## yolo训练完成后获取模型的路径

在使用 **YOLOv5/YOLOv8（Ultralytics）系列模型训练完成后**，训练过程会自动将模型权重（`.pt` 文件）保存到默认的目录中。以下是详细说明：

---

## ✅ 默认保存路径

### 对于 YOLOv5：

训练完成后，权重文件通常保存在：

```
runs/train/exp/weights/best.pt
```

* `exp/` 是实验编号目录，如果已有 `exp` 目录，会自动创建 `exp1`, `exp2` ...
* `best.pt`：是验证集上性能最好的模型（自动保存）
* `last.pt`：是最后一个 epoch 保存的模型（也会保存）

### 对于 YOLOv8（Ultralytics）：

YOLOv8 使用 `ultralytics` 包训练，默认保存位置类似：

```
runs/detect/train/weights/best.pt
```

或者如果你运行的是实例分割任务（`task=segment`）：

```
runs/segment/train/weights/best.pt
```

---

## 🛠️ 如何自定义保存路径

你可以通过 `--project` 和 `--name` 参数指定保存位置，例如：

```bash
yolo task=detect mode=train model=yolov8n.pt data=data.yaml epochs=100 project=my_project name=my_exp
```

这将保存到：

```
my_project/my_exp/weights/best.pt
```

---

## 🔍 查找路径技巧（Python 示例）

如果你用的是脚本训练，可以这样捕获路径：

```python
from ultralytics import YOLO

model = YOLO("yolov8n.yaml")
results = model.train(data="data.yaml", epochs=50)
print("Model saved at:", model.ckpt_path)
```

或者查看 `results.save_dir` 属性获取整个输出目录路径：

```python
print("Results saved in:", results.save_dir)
```

---

如果你告诉我你是用哪种 YOLO（v5 / v8 / 其他），我可以给你更精确的文件路径或查找方式。

## AMP: running Automatic Mixed Precision (AMP) checks...

+ 这个 AMP 只是为了在训练开始之前进行AMP检查，以决定是否允许此训练模式。
+ 官方回答：
  + YOLOv8n is used for AMPP checks prior to training start to decide whether to allow this training mode(as your console printout clearly displays). Your YOLOv8m model will train as normal.